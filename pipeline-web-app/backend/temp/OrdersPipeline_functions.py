"""
ETL Functions for OrdersPipeline
Auto-generated by Acceleo
"""

import pandas as pd
import json
from typing import Dict, Any

import psycopg2
from sqlalchemy import create_engine



# ====================
# Extract Functions
# ====================

def extract_postgres(**kwargs):
    """Extract data from postgres source"""
    import psycopg2
    
    params = kwargs
    conn = psycopg2.connect(
        host="localhost",
        port=5432,
        database="mydb",
        user="user",
        password="pass"
    )
    
    query = f"SELECT * FROM {params['table']}"
    df = pd.read_sql(query, conn)
    conn.close()
    
    print(f"‚úÖ Extracted {len(df)} rows from {params['table']}")
    return df.to_json(orient='records')




# ====================
# Transform Functions
# ====================

def transform_clean(ti, **kwargs):
    """Transform data - Transform data"""
    # Get data from previous task
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids[0] if upstream_task_ids else None
    if upstream_task_id:
        data_json = ti.xcom_pull(task_ids=upstream_task_id)
    else:
        raise ValueError("No upstream task found")
    
    df = pd.read_json(data_json)
    params = kwargs
    
    initial_rows = len(df)
    
    # Apply transformations
    if params.get('remove_nulls', False):
        df = df.dropna()
        print(f"üßπ Removed {initial_rows - len(df)} rows with nulls")
    
    if params.get('trim', False):
        string_cols = df.select_dtypes(include=[object]).columns
        for col in string_cols:
            df[col] = df[col].str.strip()
        print(f"‚úÇÔ∏è  Trimmed whitespace from string columns")
    
    if params.get('deduplicate', False):
        before = len(df)
        df = df.drop_duplicates()
        print(f"üîç Removed {before - len(df)} duplicate rows")
    
    print(f"‚úÖ Transformation complete: {len(df)} rows")
    return df.to_json(orient='records')




# ====================
# Load Functions
# ====================

def load_warehouse(ti, **kwargs):
    """Load data to postgres destination"""
    # Get transformed data
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids[0] if upstream_task_ids else None
    data_json = ti.xcom_pull(task_ids=upstream_task_id)
    df = pd.read_json(data_json)
    
    params = kwargs
    
    from sqlalchemy import create_engine
    
    engine = create_engine(
        f"postgresql://user:pass@"
        f"localhsot:5432/analytics"
    )
    
    df.to_sql(params['table'], engine, if_exists='append', index=False)
    print(f"‚úÖ Loaded {len(df)} rows to {params['table']}")



