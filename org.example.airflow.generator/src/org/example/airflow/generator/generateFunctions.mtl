[comment encoding = UTF-8 /]
[module generateFunctions('http://www.example.org/airflowpipeline')]

[template public generateFunctions(dag : DAG)]
[file (dag.dagId + '_functions.py', false, 'UTF-8')]
"""
ETL Functions for [dag.dagId/]
Auto-generated by Acceleo
"""

import pandas as pd
import json
from typing import Dict, Any

[generateImports(dag)/]

# ====================
# Extract Functions
# ====================

[for (op : PythonOperator | dag.operators->select(opcode = 'extract'))]
[generateExtractFunction(op)/]

[/for]

# ====================
# Transform Functions
# ====================

[for (op : PythonOperator | dag.operators->select(opcode.startsWith('transform')))]
[generateTransformFunction(op)/]

[/for]

# ====================
# Load Functions
# ====================

[for (op : PythonOperator | dag.operators->select(opcode = 'load'))]
[generateLoadFunction(op)/]

[/for]
[/file]
[/template]

[comment Generate necessary imports based on operators /]
[template public generateImports(dag : DAG)]
[if (dag.operators->exists(op : PythonOperator | op.connection.connType = 'postgres'))]
import psycopg2
from sqlalchemy import create_engine
[/if]
[if (dag.operators->exists(op : PythonOperator | op.connection.connType = 'mongo'))]
from pymongo import MongoClient
[/if]

[/template]

[comment Extract Function Template /]
[template public generateExtractFunction(op : PythonOperator)]
def [op.pythonCallable/](**kwargs):
    """Extract data from [op.connection.connType/] source"""
    [if (op.connection.connType = 'postgres')]
    import psycopg2
    
    params = kwargs
    conn = psycopg2.connect(
        host="[op.connection.host/]",
        port=[op.connection.port/],
        database="[op.connection.schema/]",
        user="[op.connection.login/]",
        password="[op.connection.password/]"
    )
    
    query = f"SELECT * FROM {params['['/]table[']'/]}"
    df = pd.read_sql(query, conn)
    conn.close()
    
    print(f"‚úÖ Extracted {len(df)} rows from {params['['/]table[']'/]}")
    return df.to_json(orient='records')
    [elseif (op.connection.connType = 'mongo')]
    from pymongo import MongoClient
    
    params = kwargs
    client = MongoClient("[op.connection.host/]", [op.connection.port/])
    db = client['['/][op.connection.schema/][']'/]
    collection = db['['/]{params['['/]collection[']'/]}[']'/]
    
    data = list(collection.find())
    # Remove MongoDB _id field
    for doc in data:
        if '_id' in doc:
            doc.pop('_id')
    
    df = pd.DataFrame(data)
    
    print(f"‚úÖ Extracted {len(df)} documents from {params['['/]collection[']'/]}")
    return df.to_json(orient='records')
    [else]
    # TODO: Implement extraction for [op.connection.connType/]
    raise NotImplementedError("Extraction not implemented for [op.connection.connType/]")
    [/if]

[/template]

[comment Transform Function Template /]
[template public generateTransformFunction(op : PythonOperator)]
def [op.pythonCallable/](ti, **kwargs):
    """Transform data - [op.description/]"""
    [if (op.opcode = 'transform_clean')]
    # Get data from previous task
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids['['/]0[']'/] if upstream_task_ids else None
    if upstream_task_id:
        data_json = ti.xcom_pull(task_ids=upstream_task_id)
    else:
        raise ValueError("No upstream task found")
    
    df = pd.read_json(data_json)
    params = kwargs
    
    initial_rows = len(df)
    
    # Apply transformations
    if params.get('remove_nulls', False):
        df = df.dropna()
        print(f"üßπ Removed {initial_rows - len(df)} rows with nulls")
    
    if params.get('trim', False):
        string_cols = df.select_dtypes(include=['['/]object[']'/]).columns
        for col in string_cols:
            df['['/]{col}[']'/] = df['['/]{col}[']'/].str.strip()
        print(f"‚úÇÔ∏è  Trimmed whitespace from string columns")
    
    if params.get('deduplicate', False):
        before = len(df)
        df = df.drop_duplicates()
        print(f"üîç Removed {before - len(df)} duplicate rows")
    
    print(f"‚úÖ Transformation complete: {len(df)} rows")
    return df.to_json(orient='records')
    [elseif (op.opcode = 'transform_filter')]
    # Filter transformation
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids['['/]0[']'/] if upstream_task_ids else None
    data_json = ti.xcom_pull(task_ids=upstream_task_id)
    df = pd.read_json(data_json)
    
    params = kwargs
    condition = params['['/]condition[']'/]
    
    # Apply filter
    df_filtered = df.query(condition)
    print(f"üîç Filtered from {len(df)} to {len(df_filtered)} rows")
    return df_filtered.to_json(orient='records')
    [elseif (op.opcode = 'transform_aggregate')]
    # Aggregate transformation
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids['['/]0[']'/] if upstream_task_ids else None
    data_json = ti.xcom_pull(task_ids=upstream_task_id)
    df = pd.read_json(data_json)
    
    params = kwargs
    group_by = params['['/]group_by[']'/]
    aggregation = params['['/]aggregation[']'/]
    
    # Apply aggregation
    df_agg = df.groupby(group_by).agg(aggregation)
    print(f"üìä Aggregated from {len(df)} to {len(df_agg)} rows")
    return df_agg.to_json(orient='records')
    [else]
    # Generic transform
    raise NotImplementedError("Transform type [op.opcode/] not implemented")
    [/if]

[/template]

[comment Load Function Template /]
[template public generateLoadFunction(op : PythonOperator)]
def [op.pythonCallable/](ti, **kwargs):
    """Load data to [op.connection.connType/] destination"""
    # Get transformed data
    upstream_task_ids = list(ti.task.upstream_task_ids)
    upstream_task_id = upstream_task_ids['['/]0[']'/] if upstream_task_ids else None
    data_json = ti.xcom_pull(task_ids=upstream_task_id)
    df = pd.read_json(data_json)
    
    params = kwargs
    
    [if (op.connection.connType = 'postgres')]
    from sqlalchemy import create_engine
    
    engine = create_engine(
        f"postgresql://[op.connection.login/]:[op.connection.password/]@"
        f"[op.connection.host/]:[op.connection.port/]/[op.connection.schema/]"
    )
    
    df.to_sql(params['['/]table[']'/], engine, if_exists='append', index=False)
    print(f"‚úÖ Loaded {len(df)} rows to {params['['/]table[']'/]}")
    [else]
    # TODO: Implement loading for [op.connection.connType/]
    raise NotImplementedError("Loading not implemented for [op.connection.connType/]")
    [/if]

[/template]
